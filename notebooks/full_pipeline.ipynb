{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb02aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: c:\\Users\\Elham.Ahmadi\\OneDrive - Kühne Logistics University\\Desktop\\Paper Two\\ML with Applications\\Github\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Always run from repo root\n",
    "repo_root = Path.cwd().parent\n",
    "os.chdir(repo_root)\n",
    "\n",
    "print(\"Working directory set to:\", Path.cwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Github Version\n",
    "# ============================================================\n",
    "# FULL PIPELINE (FROM SCRATCH) – HOURLY WEATHER → DELAY\n",
    "#   (CAUSAL ML + ROBUSTNESS + CLUSTER-SE + STABILITY + GATE MT)\n",
    "#\n",
    "# DATA:\n",
    "#   data/df_hourly_merged_clean.csv\n",
    "#\n",
    "# TREATMENTS (binary):\n",
    "#   T_temp_severe, T_wind_severe, T_rain_severe, Snow_Any\n",
    "#\n",
    "# OUTCOME:\n",
    "#   Delta_Arrival (minutes, + = more delay, - = early)\n",
    "#\n",
    "# METHODS:\n",
    "#   * PLR-DML (econml.LinearDML) + CausalForestDML\n",
    "#   * Crump-style overlap trimming via propensity scores\n",
    "#   * Propensity histograms + ECDF (before / after trimming)\n",
    "#   * SMD balance checks before/after trimming\n",
    "#   * IATE scatter, GATEs by line, weekday, month, hour (Holm adjustment)\n",
    "#   * OLS-based partial-R² sensitivity (Cinelli & Hazlett style inputs)\n",
    "#   * Robustness value RV_zero\n",
    "#   * Outcome robustness:\n",
    "#       - Treated vs control delay distributions\n",
    "#       - Mean, median, 5%-trimmed mean differences\n",
    "#       - Winsorized mean differences (1%, 2%)\n",
    "#       - Quantile treatment effects (τ = 0.5, 0.75, 0.9)\n",
    "#   * Robustness frontier plot in R²-space\n",
    "#   * Cluster-robust inference via manual PLR-DML (line-day clusters)\n",
    "#   * Multi-seed cross-fit stability analysis (ATE per seed)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "\n",
    "from econml.dml import LinearDML, CausalForestDML\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 0) OUTPUTS + GLOBAL STYLE\n",
    "# ============================================================\n",
    "\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
    "TAB_DIR = OUTPUT_DIR / \"tables\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAVE_FIGS = True\n",
    "SAVE_TABLES = True\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (8, 4),\n",
    "    \"figure.dpi\": 150,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 11,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"grid.linestyle\": \":\",\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "def _safe_name(s: str) -> str:\n",
    "    \"\"\"Make a filesystem-safe name.\"\"\"\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s.strip())\n",
    "    s = re.sub(r\"[^a-zA-Z0-9_\\-\\.]+\", \"\", s)\n",
    "    return s[:180] if len(s) > 180 else s\n",
    "\n",
    "\n",
    "def save_fig(fig, filename: str, subdir: str = None):\n",
    "    \"\"\"Save matplotlib figure into outputs/figures (optional subdir).\"\"\"\n",
    "    if not SAVE_FIGS:\n",
    "        return None\n",
    "    out_dir = FIG_DIR if subdir is None else (FIG_DIR / subdir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / filename\n",
    "    fig.savefig(path, bbox_inches=\"tight\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def save_table(df: pd.DataFrame, name: str, to_latex: bool = True, index: bool = False):\n",
    "    \"\"\"Save DataFrame into outputs/tables as CSV (+ optional LaTeX).\"\"\"\n",
    "    if not SAVE_TABLES:\n",
    "        return None, None\n",
    "\n",
    "    base = _safe_name(name)\n",
    "    csv_path = TAB_DIR / f\"{base}.csv\"\n",
    "    df.to_csv(csv_path, index=index)\n",
    "\n",
    "    tex_path = None\n",
    "    if to_latex:\n",
    "        tex_path = TAB_DIR / f\"{base}.tex\"\n",
    "        try:\n",
    "            df.to_latex(tex_path, index=index, escape=True)\n",
    "        except Exception:\n",
    "            with open(tex_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(df.to_string(index=index))\n",
    "    return csv_path, tex_path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1) LOAD + BASIC CLEANING\n",
    "# ============================================================\n",
    "\n",
    "def load_and_clean(DATA_PATH: Path, y_col: str = \"Delta_Arrival\") -> pd.DataFrame:\n",
    "    print(\"=== Loading merged hourly dataset ===\")\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(\"Merged hourly df shape (raw):\", df.shape)\n",
    "\n",
    "    # drop problematic columns if exist\n",
    "    for col_to_drop in [\"Precipitation_Form_Hourly\"]:\n",
    "        if col_to_drop in df.columns:\n",
    "            df = df.drop(columns=[col_to_drop])\n",
    "            print(f\"Dropped column: {col_to_drop}\")\n",
    "\n",
    "    # parse dates\n",
    "    if \"Datum_x\" not in df.columns:\n",
    "        raise ValueError(\"Datum_x is required in the merged dataset.\")\n",
    "    df[\"Datum_x\"] = pd.to_datetime(df[\"Datum_x\"], errors=\"coerce\")\n",
    "\n",
    "    if \"Arrival_Time\" in df.columns:\n",
    "        df[\"Arrival_Time\"] = pd.to_datetime(df[\"Arrival_Time\"], errors=\"coerce\")\n",
    "\n",
    "    # outcome numeric\n",
    "    df[y_col] = pd.to_numeric(df[y_col], errors=\"coerce\")\n",
    "\n",
    "    # outlier removal (99.9% quantile of |delay|)\n",
    "    abs_delay = df[y_col].abs()\n",
    "    q_999 = abs_delay.quantile(0.999)\n",
    "    max_delay = float(q_999)\n",
    "    mask_bad = abs_delay > max_delay\n",
    "\n",
    "    print(f\"\\n99.9% quantile of |{y_col}|: {max_delay:.2f} minutes\")\n",
    "    print(f\"Rows with |{y_col}| > {max_delay:.2f} min (set to NaN):\", int(mask_bad.sum()))\n",
    "    df.loc[mask_bad, y_col] = np.nan\n",
    "\n",
    "    df = df[df[y_col].notna()]\n",
    "    df = df[df[\"Datum_x\"].notna()]\n",
    "    print(\"After basic cleaning, shape:\", df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2) TIME COVARIATES + CLUSTERS + COVARIATE SET\n",
    "# ============================================================\n",
    "\n",
    "def add_time_covariates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[\"Year\"] = d[\"Datum_x\"].dt.year\n",
    "    d[\"Month\"] = d[\"Datum_x\"].dt.month\n",
    "    d[\"Weekday\"] = d[\"Datum_x\"].dt.weekday  # 0=Mon\n",
    "\n",
    "    if \"Arrival_Time\" in d.columns and d[\"Arrival_Time\"].notna().any():\n",
    "        d[\"Hour\"] = d[\"Arrival_Time\"].dt.hour\n",
    "    elif \"Hour\" in d.columns:\n",
    "        d[\"Hour\"] = pd.to_numeric(d[\"Hour\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    else:\n",
    "        d[\"Hour\"] = 0\n",
    "\n",
    "    d[\"month_sin\"] = np.sin(2 * np.pi * d[\"Month\"] / 12)\n",
    "    d[\"month_cos\"] = np.cos(2 * np.pi * d[\"Month\"] / 12)\n",
    "\n",
    "    # planned departure cyclic\n",
    "    if \"Planned_Departure\" in d.columns:\n",
    "        d[\"Planned_Departure_dt\"] = pd.to_datetime(d[\"Planned_Departure\"], errors=\"coerce\")\n",
    "        dep_minutes = (\n",
    "            d[\"Planned_Departure_dt\"].dt.hour * 60\n",
    "            + d[\"Planned_Departure_dt\"].dt.minute\n",
    "            + d[\"Planned_Departure_dt\"].dt.second / 60.0\n",
    "        )\n",
    "        d[\"Dep_Minutes\"] = dep_minutes\n",
    "        d[\"dep_sin\"] = np.sin(2 * np.pi * dep_minutes / 1440.0)\n",
    "        d[\"dep_cos\"] = np.cos(2 * np.pi * dep_minutes / 1440.0)\n",
    "        print(\"\\nPlanned departure time converted to cyclic features (dep_sin, dep_cos).\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: 'Planned_Departure' column not found. No dep_sin/dep_cos created.\")\n",
    "\n",
    "    # cluster: line-day\n",
    "    if \"Line_Original\" in d.columns:\n",
    "        d[\"Cluster_LineDay\"] = d[\"Line_Original\"].astype(str) + \"_\" + d[\"Datum_x\"].dt.date.astype(str)\n",
    "    else:\n",
    "        d[\"Cluster_LineDay\"] = d[\"Datum_x\"].dt.date.astype(str)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def build_covariates(df: pd.DataFrame):\n",
    "    base_covariates = [\n",
    "        \"Year\", \"Month\", \"Weekday\", \"Hour\",\n",
    "        \"Distance\", \"Holidays_Hessen\",\n",
    "        \"month_sin\", \"month_cos\",\n",
    "        \"dep_sin\", \"dep_cos\",\n",
    "        \"Station_ID\", \"Line_Original\", \"Direction\",\n",
    "    ]\n",
    "    base_covariates = [c for c in base_covariates if c in df.columns]\n",
    "\n",
    "    num_features = [c for c in [\n",
    "        \"Hour\", \"Distance\", \"Holidays_Hessen\",\n",
    "        \"month_sin\", \"month_cos\", \"dep_sin\", \"dep_cos\"\n",
    "    ] if c in base_covariates]\n",
    "    cat_features = [c for c in base_covariates if c not in num_features]\n",
    "\n",
    "    print(\"\\nNumeric features:\", num_features)\n",
    "    print(\"Categorical features:\", cat_features)\n",
    "    print(\"Cluster variable: Cluster_LineDay\")\n",
    "\n",
    "    return base_covariates, num_features, cat_features, \"Cluster_LineDay\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3) CONSTRUCT SEVERE WEATHER TREATMENTS (BASELINE)\n",
    "# ============================================================\n",
    "\n",
    "def add_baseline_treatments(df: pd.DataFrame):\n",
    "    d = df.copy()\n",
    "\n",
    "    # Temperature tails 5% / 95%\n",
    "    if \"Temperature_Mean_Hourly\" not in d.columns:\n",
    "        raise ValueError(\"Temperature_Mean_Hourly missing from dataset.\")\n",
    "    temp = pd.to_numeric(d[\"Temperature_Mean_Hourly\"], errors=\"coerce\")\n",
    "    q_temp_low = temp.quantile(0.05)\n",
    "    q_temp_high = temp.quantile(0.95)\n",
    "    d[\"T_temp_severe\"] = np.where((temp <= q_temp_low) | (temp >= q_temp_high), 1, 0).astype(int)\n",
    "\n",
    "    print(\"\\nTemperature (hourly) extreme thresholds:\")\n",
    "    print(f\" q_low (0.05) = {q_temp_low:.2f} °C\")\n",
    "    print(f\" q_high(0.95) = {q_temp_high:.2f} °C\")\n",
    "    print(\"T_temp_severe value counts:\")\n",
    "    print(d[\"T_temp_severe\"].value_counts())\n",
    "\n",
    "    # Wind 90th percentile\n",
    "    if \"Wind_Speed_Hourly\" not in d.columns:\n",
    "        raise ValueError(\"Wind_Speed_Hourly missing from dataset.\")\n",
    "    wind = pd.to_numeric(d[\"Wind_Speed_Hourly\"], errors=\"coerce\")\n",
    "    wind_thr = float(wind.quantile(0.90))\n",
    "    d[\"T_wind_severe\"] = np.where(wind >= wind_thr, 1, 0).astype(int)\n",
    "\n",
    "    print(\"\\nWind (hourly) severe threshold:\")\n",
    "    print(\" Threshold type: quantile-90\")\n",
    "    print(f\" wind_thr = {wind_thr:.2f} m/s\")\n",
    "    print(\"T_wind_severe value counts:\")\n",
    "    print(d[\"T_wind_severe\"].value_counts())\n",
    "\n",
    "    # Rain 90th on positives, min 0.1\n",
    "    if \"Rainfall_Height_Hourly\" not in d.columns:\n",
    "        raise ValueError(\"Rainfall_Height_Hourly missing from dataset.\")\n",
    "    rain = pd.to_numeric(d[\"Rainfall_Height_Hourly\"], errors=\"coerce\").fillna(0.0)\n",
    "    rain_pos = rain[rain > 0]\n",
    "    if len(rain_pos) > 100:\n",
    "        rain_thr = float(max(rain_pos.quantile(0.90), 0.1))\n",
    "    else:\n",
    "        rain_thr = 2.0\n",
    "    d[\"T_rain_severe\"] = np.where(rain >= rain_thr, 1, 0).astype(int)\n",
    "\n",
    "    print(\"\\nRain (hourly) severe threshold:\")\n",
    "    print(\" Threshold type: quantile-90 on positive values (min 0.1 mm/h)\")\n",
    "    print(f\" rain_thr = {rain_thr:.2f} mm/h\")\n",
    "    print(\"T_rain_severe value counts:\")\n",
    "    print(d[\"T_rain_severe\"].value_counts())\n",
    "\n",
    "    # Snow any\n",
    "    if \"Snow_Depth\" in d.columns:\n",
    "        snow_depth = pd.to_numeric(d[\"Snow_Depth\"], errors=\"coerce\").fillna(0.0)\n",
    "        d[\"Snow_Any\"] = np.where(snow_depth > 0, 1, 0).astype(int)\n",
    "    else:\n",
    "        d[\"Snow_Any\"] = 0\n",
    "\n",
    "    print(\"\\nSnow_Any from Snow_Depth > 0:\")\n",
    "    print(pd.Series(d[\"Snow_Any\"]).value_counts())\n",
    "\n",
    "    thresholds = {\n",
    "        \"q_temp_low\": float(q_temp_low),\n",
    "        \"q_temp_high\": float(q_temp_high),\n",
    "        \"wind_thr\": float(wind_thr),\n",
    "        \"rain_thr\": float(rain_thr),\n",
    "    }\n",
    "    return d, thresholds\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4) GLOBAL MISSINGNESS CHECK\n",
    "# ============================================================\n",
    "\n",
    "def global_missingness(df: pd.DataFrame, y_col: str, cols: list):\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    print(\"\\n--- GLOBAL MISSINGNESS CHECK ---\")\n",
    "    print(\"Total rows:\", len(df))\n",
    "    print(\"Non-missing outcome:\", df[y_col].notna().mean())\n",
    "    non_missing_share = df[cols].notna().mean()\n",
    "    print(\"\\nNon-missing share per column:\")\n",
    "    print(non_missing_share)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5) CRUMP-STYLE ALPHA SELECTION\n",
    "# ============================================================\n",
    "\n",
    "def choose_alpha_crump(pscores, T, alphas, min_treated=100, min_control=100):\n",
    "    \"\"\"\n",
    "    Crump-style alpha selection:\n",
    "    - keep p in [alpha, 1-alpha]\n",
    "    - require min_treated, min_control\n",
    "    - minimize E[1/(p(1-p))]\n",
    "    \"\"\"\n",
    "    ps = np.asarray(pscores, dtype=float)\n",
    "    T = np.asarray(T, dtype=int)\n",
    "    records = []\n",
    "\n",
    "    for a in alphas:\n",
    "        mask = (ps >= a) & (ps <= 1 - a)\n",
    "        N_kept = int(mask.sum())\n",
    "        if N_kept == 0:\n",
    "            continue\n",
    "        T_trim = T[mask]\n",
    "        N_treated = int(T_trim.sum())\n",
    "        N_control = int(N_kept - N_treated)\n",
    "        if (N_treated < min_treated) or (N_control < min_control):\n",
    "            continue\n",
    "\n",
    "        w = 1.0 / (ps[mask] * (1.0 - ps[mask]))\n",
    "        crump_obj = float(np.mean(w))\n",
    "        records.append({\n",
    "            \"alpha\": float(a),\n",
    "            \"N_kept\": N_kept,\n",
    "            \"N_treated\": N_treated,\n",
    "            \"N_control\": N_control,\n",
    "            \"Crump_obj\": crump_obj\n",
    "        })\n",
    "\n",
    "    if not records:\n",
    "        print(\"No alpha satisfies min treated/control constraints. Using alpha=0.0.\")\n",
    "        return 0.0, pd.DataFrame()\n",
    "\n",
    "    df_alpha = pd.DataFrame(records)\n",
    "    best_idx = df_alpha[\"Crump_obj\"].idxmin()\n",
    "    alpha_star = float(df_alpha.loc[best_idx, \"alpha\"])\n",
    "\n",
    "    print(\"\\nAlpha selection table (Crump-style):\")\n",
    "    print(df_alpha)\n",
    "    print(f\"\\nChosen alpha*: {alpha_star:.3f} (Crump_obj={df_alpha.loc[best_idx, 'Crump_obj']:.3f})\")\n",
    "    return alpha_star, df_alpha\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6) SMD + PROPENSITY PLOTS (HIST + ECDF)\n",
    "# ============================================================\n",
    "\n",
    "def compute_smd(df_smd, treat_col, covariates, weight_col=None):\n",
    "    \"\"\"\n",
    "    Standardized mean difference per covariate (numeric after one-hot).\n",
    "    SMD = (mean_T - mean_C) / pooled_sd\n",
    "    \"\"\"\n",
    "    W = df_smd[treat_col].values.astype(float)\n",
    "    X = df_smd[covariates]\n",
    "\n",
    "    if weight_col is None:\n",
    "        w = np.ones(len(df_smd))\n",
    "    else:\n",
    "        w = df_smd[weight_col].values.astype(float)\n",
    "\n",
    "    smd_list = []\n",
    "    for col in covariates:\n",
    "        x = X[col].values.astype(float)\n",
    "\n",
    "        w_t = w[W == 1]\n",
    "        w_c = w[W == 0]\n",
    "        x_t = x[W == 1]\n",
    "        x_c = x[W == 0]\n",
    "\n",
    "        if len(x_t) < 2 or len(x_c) < 2:\n",
    "            smd = np.nan\n",
    "        else:\n",
    "            m_t = np.average(x_t, weights=w_t)\n",
    "            m_c = np.average(x_c, weights=w_c)\n",
    "            v_t = np.average((x_t - m_t) ** 2, weights=w_t)\n",
    "            v_c = np.average((x_c - m_c) ** 2, weights=w_c)\n",
    "            pooled_sd = np.sqrt(0.5 * (v_t + v_c))\n",
    "            smd = (m_t - m_c) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "        smd_list.append((col, smd))\n",
    "\n",
    "    out = pd.DataFrame(smd_list, columns=[\"covariate\", \"SMD\"])\n",
    "    out[\"abs_SMD\"] = out[\"SMD\"].abs()\n",
    "    return out.sort_values(\"abs_SMD\", ascending=False)\n",
    "\n",
    "\n",
    "def summarize_smd_before_after(df_before, df_after, treat_col, covariates, weight_col_before=None, weight_col_after=None):\n",
    "    smd_before = compute_smd(df_before, treat_col, covariates, weight_col=weight_col_before)\n",
    "    smd_after = compute_smd(df_after, treat_col, covariates, weight_col=weight_col_after)\n",
    "    merged = smd_before.merge(smd_after, on=\"covariate\", how=\"inner\", suffixes=(\"_before\", \"_after\"))\n",
    "    return merged.sort_values(\"abs_SMD_before\", ascending=False)\n",
    "\n",
    "\n",
    "def _ecdf(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    if len(x) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    x_sorted = np.sort(x)\n",
    "    y = np.arange(1, len(x_sorted) + 1) / len(x_sorted)\n",
    "    return x_sorted, y\n",
    "\n",
    "\n",
    "def plot_propensity_hist(T, pscores, t_col):\n",
    "    T = np.asarray(T)\n",
    "    ps = np.asarray(pscores)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.hist(ps[T == 1], bins=30, alpha=0.6, density=True, label=\"Treated\",\n",
    "            edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.hist(ps[T == 0], bins=30, alpha=0.6, density=True, label=\"Control\",\n",
    "            edgecolor=\"black\", linewidth=0.4)\n",
    "\n",
    "    ax.set_title(f\"Propensity score distribution – {t_col}\")\n",
    "    ax.set_xlabel(\"Propensity score\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"propensity_hist__{_safe_name(t_col)}.png\", subdir=\"propensity\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_propensity_before_after(T, pscores, mask_trim, alpha_star, t_col):\n",
    "    T = np.asarray(T)\n",
    "    ps = np.asarray(pscores)\n",
    "    mask_trim = np.asarray(mask_trim)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.hist(ps[T == 1], bins=30, alpha=0.6, density=True, label=\"Treated\",\n",
    "            edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.hist(ps[T == 0], bins=30, alpha=0.6, density=True, label=\"Control\",\n",
    "            edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.set_title(\"Before trimming\")\n",
    "    ax.set_xlabel(\"Propensity score\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "    ps_trim = ps[mask_trim]\n",
    "    T_trim = T[mask_trim]\n",
    "    ax = axes[1]\n",
    "    ax.hist(ps_trim[T_trim == 1], bins=30, alpha=0.6, density=True, label=\"Treated (trimmed)\",\n",
    "            edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.hist(ps_trim[T_trim == 0], bins=30, alpha=0.6, density=True, label=\"Control (trimmed)\",\n",
    "            edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.set_title(f\"After trimming (α = {alpha_star:.2f})\")\n",
    "    ax.set_xlabel(\"Propensity score\")\n",
    "    ax.legend()\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axvline(alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "        ax.axvline(1 - alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    fig.suptitle(f\"Propensity score trimming – {t_col}\", y=1.03)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"propensity_before_after__{_safe_name(t_col)}.png\", subdir=\"propensity\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_ps_ecdf_before_after(T, pscores, mask_trim, alpha_star, t_col):\n",
    "    T = np.asarray(T)\n",
    "    ps = np.asarray(pscores)\n",
    "    mask_trim = np.asarray(mask_trim)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "    ax = axes[0]\n",
    "    for val, label in [(1, \"Treated\"), (0, \"Control\")]:\n",
    "        xs, ys = _ecdf(ps[T == val])\n",
    "        if len(xs) > 0:\n",
    "            ax.step(xs, ys, where=\"post\", label=label)\n",
    "    ax.axvline(alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.axvline(1 - alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_title(\"ECDF – Before trimming\")\n",
    "    ax.set_xlabel(\"Propensity score\")\n",
    "    ax.set_ylabel(\"F(p)\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1]\n",
    "    ps_trim = ps[mask_trim]\n",
    "    T_trim = T[mask_trim]\n",
    "    for val, label in [(1, \"Treated (trimmed)\"), (0, \"Control (trimmed)\")]:\n",
    "        xs, ys = _ecdf(ps_trim[T_trim == val])\n",
    "        if len(xs) > 0:\n",
    "            ax.step(xs, ys, where=\"post\", label=label)\n",
    "    ax.axvline(alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.axvline(1 - alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_title(\"ECDF – After trimming\")\n",
    "    ax.set_xlabel(\"Propensity score\")\n",
    "    ax.legend()\n",
    "\n",
    "    fig.suptitle(f\"Propensity ECDFs – {t_col}\", y=1.03)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"propensity_ecdf__{_safe_name(t_col)}.png\", subdir=\"propensity\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 7) GATE / IATE PLOTS + MULTIPLE TESTING (HOLM on PLR)\n",
    "# ============================================================\n",
    "\n",
    "month_labels = {1:\"Jan\",2:\"Feb\",3:\"Mar\",4:\"Apr\",5:\"May\",6:\"Jun\",7:\"Jul\",8:\"Aug\",9:\"Sep\",10:\"Oct\",11:\"Nov\",12:\"Dec\"}\n",
    "weekday_labels = {0:\"Mon\",1:\"Tue\",2:\"Wed\",3:\"Thu\",4:\"Fri\",5:\"Sat\",6:\"Sun\"}\n",
    "\n",
    "\n",
    "def plot_iate_scatter(tau_plr, tau_cf, title, t_col):\n",
    "    tau_plr = np.asarray(tau_plr)\n",
    "    tau_cf = np.asarray(tau_cf)\n",
    "    n = len(tau_plr)\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    mean_plr = float(np.mean(tau_plr))\n",
    "    mean_cf = float(np.mean(tau_cf))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    ax.scatter(idx, tau_plr, s=6, alpha=0.4, label=f\"IATE PLR-DML (mean = {mean_plr:.2f})\")\n",
    "    ax.scatter(idx, tau_cf, s=6, alpha=0.4, label=f\"IATE CF-DML (mean = {mean_cf:.2f})\")\n",
    "    ax.axhline(mean_plr, linestyle=\"--\", linewidth=1)\n",
    "    ax.axhline(mean_cf, linestyle=\":\", linewidth=1)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Observation (trimmed sample)\")\n",
    "    ax.set_ylabel(\"Individual treatment effect (minutes)\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"iate_scatter__{_safe_name(t_col)}.png\", subdir=\"iates\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def compute_gate_two(df_gate, group_col, col_plr=\"IATE_PLR\", col_cf=\"IATE_CF\"):\n",
    "    tab_plr = df_gate.groupby(group_col)[col_plr].agg([\"mean\", \"count\", \"std\"]).rename(\n",
    "        columns={\"mean\":\"GATE_PLR\",\"count\":\"N_PLR\",\"std\":\"SD_PLR\"}\n",
    "    )\n",
    "    tab_cf = df_gate.groupby(group_col)[col_cf].agg([\"mean\", \"count\", \"std\"]).rename(\n",
    "        columns={\"mean\":\"GATE_CF\",\"count\":\"N_CF\",\"std\":\"SD_CF\"}\n",
    "    )\n",
    "    return tab_plr.join(tab_cf, how=\"inner\")\n",
    "\n",
    "\n",
    "def adjust_gate_pvalues(gate_table):\n",
    "    gate = gate_table.copy()\n",
    "    se = gate[\"SD_PLR\"] / np.sqrt(gate[\"N_PLR\"])\n",
    "    gate[\"SE_PLR\"] = se\n",
    "    gate[\"t_PLR\"] = np.where(se > 0, gate[\"GATE_PLR\"] / se, np.nan)\n",
    "    gate[\"p_PLR\"] = 2 * (1 - norm.cdf(np.abs(gate[\"t_PLR\"])))\n",
    "\n",
    "    # Holm adjustment\n",
    "    p = gate[\"p_PLR\"].values\n",
    "    m = np.sum(~np.isnan(p))\n",
    "    adj = np.full_like(p, np.nan, dtype=float)\n",
    "\n",
    "    if m > 0:\n",
    "        order = np.argsort(p)\n",
    "        ranked_p = p[order]\n",
    "        for i, p_val in enumerate(ranked_p):\n",
    "            if np.isnan(p_val):\n",
    "                adj[order[i]] = np.nan\n",
    "            else:\n",
    "                adj[order[i]] = min((m - i) * p_val, 1.0)\n",
    "\n",
    "    gate[\"p_PLR_holm\"] = adj\n",
    "    return gate\n",
    "\n",
    "\n",
    "def plot_gate_two_bars(gate_table, group_col, title, t_col, is_month=False, is_weekday=False):\n",
    "    if gate_table.shape[0] == 0:\n",
    "        print(f\"[plot_gate_two_bars] No groups available for {group_col}; skipping plot.\")\n",
    "        return None\n",
    "\n",
    "    gate = gate_table.copy()\n",
    "    idx = gate.index\n",
    "    if is_month:\n",
    "        gate.index = [month_labels.get(int(i), str(i)) for i in idx]\n",
    "    elif is_weekday:\n",
    "        gate.index = [weekday_labels.get(int(i), str(i)) for i in idx]\n",
    "\n",
    "    x = np.arange(len(gate))\n",
    "    width = 0.35\n",
    "\n",
    "    se_plr = gate[\"SD_PLR\"] / np.sqrt(gate[\"N_PLR\"])\n",
    "    se_cf = gate[\"SD_CF\"] / np.sqrt(gate[\"N_CF\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    ax.bar(x - width/2, gate[\"GATE_PLR\"], width, label=\"PLR-DML\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.bar(x + width/2, gate[\"GATE_CF\"], width, label=\"CF-DML\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.errorbar(x - width/2, gate[\"GATE_PLR\"], yerr=se_plr, fmt=\"none\", ecolor=\"black\", capsize=3, linewidth=0.8)\n",
    "    ax.errorbar(x + width/2, gate[\"GATE_CF\"], yerr=se_cf, fmt=\"none\", ecolor=\"black\", capsize=3, linewidth=0.8)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Group Average Treatment Effect (minutes)\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(gate.index, rotation=0)\n",
    "    ax.legend()\n",
    "    ax.axhline(0.0, color=\"black\", linewidth=0.8)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"gate_{_safe_name(group_col)}__{_safe_name(t_col)}.png\", subdir=\"gates\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 7b) OLS PARTIAL R² SENSITIVITY (Cinelli/Hazlett-style inputs)\n",
    "# ============================================================\n",
    "\n",
    "def run_partial_r2_sensitivity(Y_trim, T_trim, X_trim, t_col):\n",
    "    Y_trim = np.asarray(Y_trim, dtype=float)\n",
    "    T_trim = np.asarray(T_trim, dtype=float)\n",
    "    X_trim = np.asarray(X_trim, dtype=float)\n",
    "\n",
    "    # Y ~ T + X\n",
    "    X_ols_full = np.column_stack([T_trim, X_trim])\n",
    "    X_ols_full = sm.add_constant(X_ols_full)\n",
    "    ols_full = sm.OLS(Y_trim, X_ols_full).fit()\n",
    "    beta_T = float(ols_full.params[1])\n",
    "    se_T = float(ols_full.bse[1])\n",
    "    t_T = float(ols_full.tvalues[1])\n",
    "    p_T = float(ols_full.pvalues[1])\n",
    "    df_resid = float(ols_full.df_resid)\n",
    "\n",
    "    partial_R2_TY_given_X = float((t_T**2) / (t_T**2 + df_resid)) if np.isfinite(t_T) else np.nan\n",
    "\n",
    "    # Y ~ X\n",
    "    X_only = sm.add_constant(X_trim)\n",
    "    ols_y_x = sm.OLS(Y_trim, X_only).fit()\n",
    "    R2_Y_X = float(ols_y_x.rsquared)\n",
    "\n",
    "    # T ~ X\n",
    "    ols_t_x = sm.OLS(T_trim, X_only).fit()\n",
    "    R2_T_X = float(ols_t_x.rsquared)\n",
    "\n",
    "    print(f\"\\n=== Partial R² sensitivity (OLS, trimmed sample) – {t_col} ===\")\n",
    "    print(f\" beta_T (OLS, Y ~ T + X) : {beta_T:.3f}\")\n",
    "    print(f\" SE(beta_T) : {se_T:.3f}\")\n",
    "    print(f\" t-statistic : {t_T:.3f}\")\n",
    "    print(f\" p-value : {p_T:.3f}\")\n",
    "    print(f\" Partial R² T–Y | X : {partial_R2_TY_given_X:.4f}\")\n",
    "    print(f\" R²(Y ~ X) : {R2_Y_X:.4f}\")\n",
    "    print(f\" R²(T ~ X) : {R2_T_X:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"beta_T_ols\": beta_T,\n",
    "        \"se_T_ols\": se_T,\n",
    "        \"t_T_ols\": t_T,\n",
    "        \"p_T_ols\": p_T,\n",
    "        \"partial_R2_TY_given_X\": partial_R2_TY_given_X,\n",
    "        \"R2_Y_X\": R2_Y_X,\n",
    "        \"R2_T_X\": R2_T_X,\n",
    "        \"df_resid\": df_resid\n",
    "    }\n",
    "\n",
    "\n",
    "def cinelli_robustness_value(t_stat, df_resid, q=1.0):\n",
    "    \"\"\"\n",
    "    A compact RV-style function (spirit of Cinelli & Hazlett robustness value).\n",
    "    Used as a scalar summary; frontier plot below is the visual.\n",
    "    \"\"\"\n",
    "    if not np.isfinite(t_stat) or df_resid <= 0:\n",
    "        return np.nan\n",
    "    f_q = q * abs(t_stat) / np.sqrt(df_resid)\n",
    "    rv = 0.5 * (np.sqrt(f_q**4 + 4 * f_q**2) - f_q**2)\n",
    "    return float(rv)\n",
    "\n",
    "\n",
    "def compute_bias_adjusted(beta_hat, se_hat, df_resid, r2yu_tw, r2tu_w):\n",
    "    r2yu_tw = np.asarray(r2yu_tw, dtype=float)\n",
    "    r2tu_w = np.asarray(r2tu_w, dtype=float)\n",
    "    r2tu_w = np.clip(r2tu_w, 1e-8, 1 - 1e-8)\n",
    "    r2yu_tw = np.clip(r2yu_tw, 0.0, 1 - 1e-8)\n",
    "\n",
    "    sign_beta = np.sign(beta_hat) if beta_hat != 0 else 1.0\n",
    "    bias_factor = np.sqrt(r2yu_tw * r2tu_w / (1.0 - r2tu_w))\n",
    "    bias = sign_beta * bias_factor * se_hat * np.sqrt(df_resid)\n",
    "    return beta_hat - bias\n",
    "\n",
    "\n",
    "def plot_robustness_frontier(t_col, beta_hat, se_hat, df_resid, partial_R2_TY_given_X, R2_T_X, max_r2=0.2):\n",
    "    r2_tu_vals = np.linspace(0.0, max_r2, 101)\n",
    "    r2_yu_vals = np.linspace(0.0, max_r2, 101)\n",
    "    R2_TU, R2_YU = np.meshgrid(r2_tu_vals, r2_yu_vals)\n",
    "\n",
    "    beta_adj = compute_bias_adjusted(beta_hat, se_hat, df_resid, R2_YU, R2_TU)\n",
    "    t_adj = beta_adj / se_hat\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 5))\n",
    "    cf = ax.contourf(R2_TU, R2_YU, np.abs(t_adj), levels=30, alpha=0.85)\n",
    "    cbar = fig.colorbar(cf, ax=ax)\n",
    "    cbar.set_label(r\"$|t_{\\mathrm{adj}}|$\")\n",
    "\n",
    "    for lev, style in [(2.0, \"--\"), (0.0, \"-.\")]:\n",
    "        cs = ax.contour(R2_TU, R2_YU, np.abs(t_adj), levels=[lev], colors=\"k\", linestyles=style)\n",
    "        ax.clabel(cs, inline=True, fontsize=9, fmt={lev: rf\"$|t|={lev:.0f}$\"})\n",
    "\n",
    "    if np.isfinite(partial_R2_TY_given_X):\n",
    "        ax.axhline(partial_R2_TY_given_X, color=\"red\", linestyle=\":\", lw=1.5,\n",
    "                   label=rf\"$R^2_{{Y\\sim T|X}}={partial_R2_TY_given_X:.3f}$\")\n",
    "    if np.isfinite(R2_T_X):\n",
    "        ax.axvline(R2_T_X, color=\"blue\", linestyle=\":\", lw=1.5,\n",
    "                   label=rf\"$R^2_{{T\\sim X}}={R2_T_X:.3f}$\")\n",
    "\n",
    "    ax.set_xlabel(r\"$R^2_{T\\sim U\\,|\\,X}$\")\n",
    "    ax.set_ylabel(r\"$R^2_{Y\\sim U\\,|\\,T,X}$\")\n",
    "    ax.set_title(f\"Sensitivity frontier – {t_col}\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"robustness_frontier__{_safe_name(t_col)}.png\", subdir=\"sensitivity\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 7d) OUTCOME ROBUSTNESS (DISTRIBUTIONS + ROBUST STATS + QTE)\n",
    "# ============================================================\n",
    "\n",
    "def plot_delay_distributions(Y_trim, T_trim, t_col):\n",
    "    Y_trim = np.asarray(Y_trim, dtype=float)\n",
    "    T_trim = np.asarray(T_trim, dtype=int)\n",
    "    y_t = Y_trim[T_trim == 1]\n",
    "    y_c = Y_trim[T_trim == 0]\n",
    "    if len(y_t) == 0 or len(y_c) == 0:\n",
    "        print(f\"[plot_delay_distributions] No treated or control for {t_col}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    ax = axes[0]\n",
    "    bins = 40\n",
    "    ax.hist(y_c, bins=bins, alpha=0.6, density=True, label=\"Control\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.hist(y_t, bins=bins, alpha=0.6, density=True, label=\"Treated\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.set_title(f\"Delay distribution – {t_col}\")\n",
    "    ax.set_xlabel(\"Δ arrival (minutes)\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.boxplot([y_c, y_t], labels=[\"Control\", \"Treated\"], showmeans=True)\n",
    "    ax.set_title(f\"Boxplot – {t_col}\")\n",
    "    ax.set_ylabel(\"Δ arrival (minutes)\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_fig(fig, f\"delay_distributions__{_safe_name(t_col)}.png\", subdir=\"robustness\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def _trimmed_mean(x, alpha=0.05):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    x_sorted = np.sort(x)\n",
    "    n = len(x_sorted)\n",
    "    k = int(alpha * n)\n",
    "    if 2 * k >= n:\n",
    "        return np.nan\n",
    "    return float(np.mean(x_sorted[k:n - k]))\n",
    "\n",
    "\n",
    "def _winsorize(x, lower=0.01, upper=0.99):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    if len(x) == 0:\n",
    "        return x\n",
    "    q_low, q_high = np.quantile(x, [lower, upper])\n",
    "    return np.clip(x, q_low, q_high)\n",
    "\n",
    "\n",
    "def compute_robust_effects(Y_trim, T_trim, taus=(0.5, 0.75, 0.9)):\n",
    "    Y_trim = np.asarray(Y_trim, dtype=float)\n",
    "    T_trim = np.asarray(T_trim, dtype=int)\n",
    "    y_t = Y_trim[T_trim == 1]\n",
    "    y_c = Y_trim[T_trim == 0]\n",
    "\n",
    "    stats = {}\n",
    "    if len(y_t) == 0 or len(y_c) == 0:\n",
    "        return stats, pd.DataFrame()\n",
    "\n",
    "    stats[\"mean_treated\"] = float(np.mean(y_t))\n",
    "    stats[\"mean_control\"] = float(np.mean(y_c))\n",
    "    stats[\"diff_mean\"] = stats[\"mean_treated\"] - stats[\"mean_control\"]\n",
    "\n",
    "    stats[\"median_treated\"] = float(np.median(y_t))\n",
    "    stats[\"median_control\"] = float(np.median(y_c))\n",
    "    stats[\"diff_median\"] = stats[\"median_treated\"] - stats[\"median_control\"]\n",
    "\n",
    "    tm_t = _trimmed_mean(y_t, alpha=0.05)\n",
    "    tm_c = _trimmed_mean(y_c, alpha=0.05)\n",
    "    stats[\"trimmed_mean_5_treated\"] = tm_t\n",
    "    stats[\"trimmed_mean_5_control\"] = tm_c\n",
    "    stats[\"diff_trimmed_5\"] = tm_t - tm_c\n",
    "\n",
    "    y_t_w1 = _winsorize(y_t, lower=0.01, upper=0.99)\n",
    "    y_c_w1 = _winsorize(y_c, lower=0.01, upper=0.99)\n",
    "    stats[\"winsor_1_mean_treated\"] = float(np.mean(y_t_w1))\n",
    "    stats[\"winsor_1_mean_control\"] = float(np.mean(y_c_w1))\n",
    "    stats[\"diff_winsor_1\"] = stats[\"winsor_1_mean_treated\"] - stats[\"winsor_1_mean_control\"]\n",
    "\n",
    "    y_t_w2 = _winsorize(y_t, lower=0.02, upper=0.98)\n",
    "    y_c_w2 = _winsorize(y_c, lower=0.02, upper=0.98)\n",
    "    stats[\"winsor_2_mean_treated\"] = float(np.mean(y_t_w2))\n",
    "    stats[\"winsor_2_mean_control\"] = float(np.mean(y_c_w2))\n",
    "    stats[\"diff_winsor_2\"] = stats[\"winsor_2_mean_treated\"] - stats[\"winsor_2_mean_control\"]\n",
    "\n",
    "    qte_rows = []\n",
    "    for tau in taus:\n",
    "        qt = float(np.quantile(y_t, tau))\n",
    "        qc = float(np.quantile(y_c, tau))\n",
    "        qte_rows.append({\"tau\": tau, \"Q_treated\": qt, \"Q_control\": qc, \"QTE\": qt - qc})\n",
    "\n",
    "    return stats, pd.DataFrame(qte_rows)\n",
    "\n",
    "\n",
    "def plot_robust_effects_bar(robust_stats, qte_df, t_col):\n",
    "    if robust_stats is None or len(robust_stats) == 0:\n",
    "        print(f\"[plot_robust_effects_bar] No stats for {t_col}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    effects, labels = [], []\n",
    "    for k, lab in [\n",
    "        (\"diff_mean\", \"Mean\"),\n",
    "        (\"diff_median\", \"Median\"),\n",
    "        (\"diff_trimmed_5\", \"Trimmed 5%\"),\n",
    "        (\"diff_winsor_1\", \"Winsor 1%\"),\n",
    "        (\"diff_winsor_2\", \"Winsor 2%\"),\n",
    "    ]:\n",
    "        if k in robust_stats and np.isfinite(robust_stats[k]):\n",
    "            labels.append(lab)\n",
    "            effects.append(float(robust_stats[k]))\n",
    "\n",
    "    if qte_df is not None and not qte_df.empty:\n",
    "        for tau_target in [0.5, 0.9]:\n",
    "            row = qte_df[np.isclose(qte_df[\"tau\"], tau_target)]\n",
    "            if not row.empty:\n",
    "                labels.append(f\"QTE τ={tau_target:.2f}\")\n",
    "                effects.append(float(row[\"QTE\"].iloc[0]))\n",
    "\n",
    "    if len(effects) == 0:\n",
    "        print(f\"[plot_robust_effects_bar] No finite effects to plot for {t_col}.\")\n",
    "        return None\n",
    "\n",
    "    x = np.arange(len(effects))\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.bar(x, effects, edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.axhline(0.0, color=\"black\", linewidth=0.8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=0)\n",
    "    ax.set_ylabel(\"Treatment effect on Δ arrival (minutes)\")\n",
    "    ax.set_title(f\"Robustness summary – {t_col}\\n(positive = more delay)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"robust_effects__{_safe_name(t_col)}.png\", subdir=\"robustness\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 7e) MANUAL PLR-DML WITH CLUSTER-ROBUST SE + MULTI-SEED\n",
    "# ============================================================\n",
    "\n",
    "def plr_dml_cluster_multi_seed(\n",
    "    Y_trim,\n",
    "    T_trim,\n",
    "    X_trim,\n",
    "    cluster_ids,\n",
    "    n_folds=3,\n",
    "    seeds=(1, 11, 21, 31, 41),\n",
    "    rf_y_params=None,\n",
    "    rf_t_params=None,\n",
    "    t_col=\"treatment\"\n",
    "):\n",
    "    Y = np.asarray(Y_trim, dtype=float)\n",
    "    T = np.asarray(T_trim, dtype=float)\n",
    "    X = np.asarray(X_trim, dtype=float)\n",
    "    clusters = np.asarray(cluster_ids)\n",
    "\n",
    "    if rf_y_params is None:\n",
    "        rf_y_params = dict(n_estimators=300, max_depth=12, min_samples_leaf=50, random_state=None, n_jobs=-1)\n",
    "    if rf_t_params is None:\n",
    "        rf_t_params = dict(n_estimators=300, max_depth=8, min_samples_leaf=50, random_state=None, n_jobs=-1)\n",
    "\n",
    "    records = []\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n[PLR cluster] Running residualization for seed={seed} ...\")\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        Y_tilde = np.zeros_like(Y, dtype=float)\n",
    "        T_tilde = np.zeros_like(T, dtype=float)\n",
    "\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            Y_train = Y[train_idx]\n",
    "            T_train = T[train_idx]\n",
    "\n",
    "            my = RandomForestRegressor(**{**rf_y_params, \"random_state\": seed})\n",
    "            my.fit(X_train, Y_train)\n",
    "            mY_test = my.predict(X_test)\n",
    "\n",
    "            mt = RandomForestClassifier(**{**rf_t_params, \"random_state\": seed})\n",
    "            mt.fit(X_train, T_train)\n",
    "            mT_test = mt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            Y_tilde[test_idx] = Y[test_idx] - mY_test\n",
    "            T_tilde[test_idx] = T[test_idx] - mT_test\n",
    "\n",
    "        X_ols = sm.add_constant(T_tilde)\n",
    "        model = sm.OLS(Y_tilde, X_ols)\n",
    "        res = model.fit(cov_type=\"cluster\", cov_kwds={\"groups\": clusters})\n",
    "\n",
    "        beta = float(res.params[1])\n",
    "        se_cl = float(res.bse[1])\n",
    "        t_val = float(res.tvalues[1])\n",
    "        p_val = float(res.pvalues[1])\n",
    "        ci = res.conf_int()\n",
    "        ci_low = float(ci[1, 0])\n",
    "        ci_high = float(ci[1, 1])\n",
    "\n",
    "        print(f\"[PLR cluster seed={seed}] beta={beta:.4f}, SE_cluster={se_cl:.4f}, \"\n",
    "              f\"CI=[{ci_low:.4f}, {ci_high:.4f}], p={p_val:.4f}\")\n",
    "\n",
    "        records.append({\n",
    "            \"seed\": seed,\n",
    "            \"beta_cluster\": beta,\n",
    "            \"SE_cluster\": se_cl,\n",
    "            \"CI_low_cluster\": ci_low,\n",
    "            \"CI_high_cluster\": ci_high,\n",
    "            \"t_cluster\": t_val,\n",
    "            \"p_cluster\": p_val\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(records)\n",
    "    summary = {\n",
    "        \"beta_cluster_mean\": float(results_df[\"beta_cluster\"].mean()),\n",
    "        \"beta_cluster_sd\": float(results_df[\"beta_cluster\"].std()),\n",
    "        \"beta_cluster_min\": float(results_df[\"beta_cluster\"].min()),\n",
    "        \"beta_cluster_max\": float(results_df[\"beta_cluster\"].max()),\n",
    "    }\n",
    "\n",
    "    save_table(results_df, f\"cluster_plr_multiseed__{_safe_name(t_col)}\", to_latex=False, index=False)\n",
    "\n",
    "    print(\"\\n[PLR cluster] Stability summary across seeds:\")\n",
    "    print(results_df)\n",
    "    print(\"Mean beta_cluster:\", summary[\"beta_cluster_mean\"])\n",
    "    print(\"SD beta_cluster  :\", summary[\"beta_cluster_sd\"])\n",
    "    return results_df, summary\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Core routine : now saves ALL figs + tables into outputs/\n",
    "#    - outputs/figures/<subdir>/*.png\n",
    "#    - outputs/tables/*.csv (+ optional *.tex)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# 8.0 Output folders (GitHub-friendly)\n",
    "# ----------------------------\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
    "TAB_DIR = OUTPUT_DIR / \"tables\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAVE_FIGS = True\n",
    "SAVE_TABLES = True\n",
    "\n",
    "\n",
    "def _safe_name(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s.strip())\n",
    "    s = re.sub(r\"[^a-zA-Z0-9_\\-\\.]+\", \"\", s)\n",
    "    return s[:180] if len(s) > 180 else s\n",
    "\n",
    "\n",
    "def save_fig(fig, filename: str, subdir: str = None):\n",
    "    \"\"\"Save matplotlib figure to outputs/figures (optionally in subdir).\"\"\"\n",
    "    if not SAVE_FIGS:\n",
    "        return None\n",
    "    out_dir = FIG_DIR if subdir is None else (FIG_DIR / subdir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / filename\n",
    "    fig.savefig(path, bbox_inches=\"tight\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def save_table(df: pd.DataFrame, name: str, to_latex: bool = True, index: bool = False):\n",
    "    \"\"\"Save a dataframe as CSV (+ optional LaTeX) to outputs/tables.\"\"\"\n",
    "    if not SAVE_TABLES:\n",
    "        return None, None\n",
    "    base = _safe_name(name)\n",
    "    csv_path = TAB_DIR / f\"{base}.csv\"\n",
    "    df.to_csv(csv_path, index=index)\n",
    "\n",
    "    tex_path = None\n",
    "    if to_latex:\n",
    "        tex_path = TAB_DIR / f\"{base}.tex\"\n",
    "        try:\n",
    "            df.to_latex(tex_path, index=index, escape=True)\n",
    "        except Exception:\n",
    "            with open(tex_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(df.to_string(index=index))\n",
    "    return csv_path, tex_path\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 8.1 Patch plot helpers to save figures\n",
    "# ----------------------------\n",
    "def plot_propensity_hist(T, pscores, t_col):\n",
    "    T = np.asarray(T)\n",
    "    ps = np.asarray(pscores)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.hist(ps[T == 1], bins=30, alpha=0.6, density=True,\n",
    "            label=\"Treated\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.hist(ps[T == 0], bins=30, alpha=0.6, density=True,\n",
    "            label=\"Control\", edgecolor=\"black\", linewidth=0.4)\n",
    "\n",
    "    ax.set_title(f\"Propensity score distribution – {t_col}\")\n",
    "    ax.set_xlabel(\"Propensity score\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"propensity_hist__{_safe_name(t_col)}.png\", subdir=\"propensity\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_propensity_before_after(T, pscores, mask_trim, alpha_star, t_col):\n",
    "    T = np.asarray(T)\n",
    "    ps = np.asarray(pscores)\n",
    "    mask_trim = np.asarray(mask_trim)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "    # Before trimming\n",
    "    ax = axes[0]\n",
    "    ax.hist(ps[T == 1], bins=30, alpha=0.6, density=True,\n",
    "            label=\"Treated\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.hist(ps[T == 0], bins=30, alpha=0.6, density=True,\n",
    "            label=\"Control\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.set_title(\"Before trimming\")\n",
    "    ax.set_xlabel(\"Propensity score\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "    # After trimming\n",
    "    ps_trim = ps[mask_trim]\n",
    "    T_trim = T[mask_trim]\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.hist(ps_trim[T_trim == 1], bins=30, alpha=0.6, density=True,\n",
    "            label=\"Treated (trimmed)\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.hist(ps_trim[T_trim == 0], bins=30, alpha=0.6, density=True,\n",
    "            label=\"Control (trimmed)\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.set_title(f\"After trimming (α = {alpha_star:.2f})\")\n",
    "    ax.set_xlabel(\"Propensity score\")\n",
    "    ax.legend()\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axvline(alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "        ax.axvline(1 - alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    fig.suptitle(f\"Propensity score trimming – {t_col}\", y=1.03)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"propensity_before_after__{_safe_name(t_col)}.png\", subdir=\"propensity\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def _ecdf(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    if len(x) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    x_sorted = np.sort(x)\n",
    "    y = np.arange(1, len(x_sorted) + 1) / len(x_sorted)\n",
    "    return x_sorted, y\n",
    "\n",
    "\n",
    "def plot_ps_ecdf_before_after(T, pscores, mask_trim, alpha_star, t_col):\n",
    "    \"\"\"ECDF of propensity scores by treatment (before/after trimming) + SAVE.\"\"\"\n",
    "    T = np.asarray(T)\n",
    "    ps = np.asarray(pscores)\n",
    "    mask_trim = np.asarray(mask_trim)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "    # Before trimming\n",
    "    ax = axes[0]\n",
    "    for val, label in [(1, \"Treated\"), (0, \"Control\")]:\n",
    "        xs, ys = _ecdf(ps[T == val])\n",
    "        if len(xs) > 0:\n",
    "            ax.step(xs, ys, where=\"post\", label=label)\n",
    "    ax.axvline(alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.axvline(1 - alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_title(\"ECDF – Before trimming\")\n",
    "    ax.set_xlabel(\"Propensity score\")\n",
    "    ax.set_ylabel(\"F(p)\")\n",
    "    ax.legend()\n",
    "\n",
    "    # After trimming\n",
    "    ax = axes[1]\n",
    "    ps_trim = ps[mask_trim]\n",
    "    T_trim = T[mask_trim]\n",
    "    for val, label in [(1, \"Treated (trimmed)\"), (0, \"Control (trimmed)\")]:\n",
    "        xs, ys = _ecdf(ps_trim[T_trim == val])\n",
    "        if len(xs) > 0:\n",
    "            ax.step(xs, ys, where=\"post\", label=label)\n",
    "    ax.axvline(alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.axvline(1 - alpha_star, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_title(\"ECDF – After trimming\")\n",
    "    ax.set_xlabel(\"Propensity score\")\n",
    "    ax.legend()\n",
    "\n",
    "    fig.suptitle(f\"Propensity ECDFs – {t_col}\", y=1.03)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"propensity_ecdf__{_safe_name(t_col)}.png\", subdir=\"propensity\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_gate_two_bars(gate_table, group_col, title, t_col, is_month=False, is_weekday=False):\n",
    "    if gate_table.shape[0] == 0:\n",
    "        print(f\"[plot_gate_two_bars] No groups available for {group_col}; skipping plot.\")\n",
    "        return None\n",
    "\n",
    "    gate = gate_table.copy()\n",
    "    idx = gate.index\n",
    "\n",
    "    if is_month:\n",
    "        gate.index = [month_labels.get(int(i), str(i)) for i in idx]\n",
    "    elif is_weekday:\n",
    "        gate.index = [weekday_labels.get(int(i), str(i)) for i in idx]\n",
    "\n",
    "    x = np.arange(len(gate))\n",
    "    width = 0.35\n",
    "\n",
    "    se_plr = gate[\"SD_PLR\"] / np.sqrt(gate[\"N_PLR\"])\n",
    "    se_cf = gate[\"SD_CF\"] / np.sqrt(gate[\"N_CF\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    ax.bar(x - width/2, gate[\"GATE_PLR\"], width, label=\"PLR-DML\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.bar(x + width/2, gate[\"GATE_CF\"], width, label=\"CF-DML\", edgecolor=\"black\", linewidth=0.4)\n",
    "\n",
    "    ax.errorbar(x - width/2, gate[\"GATE_PLR\"], yerr=se_plr, fmt=\"none\", ecolor=\"black\", capsize=3, linewidth=0.8)\n",
    "    ax.errorbar(x + width/2, gate[\"GATE_CF\"], yerr=se_cf, fmt=\"none\", ecolor=\"black\", capsize=3, linewidth=0.8)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Group Average Treatment Effect (minutes)\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(gate.index, rotation=0)\n",
    "    ax.legend()\n",
    "    ax.axhline(0.0, color=\"black\", linewidth=0.8)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"gate_{_safe_name(group_col)}__{_safe_name(t_col)}.png\", subdir=\"gates\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_iate_scatter(tau_plr, tau_cf, title, t_col):\n",
    "    tau_plr = np.asarray(tau_plr)\n",
    "    tau_cf = np.asarray(tau_cf)\n",
    "    n = len(tau_plr)\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    mean_plr = float(np.mean(tau_plr))\n",
    "    mean_cf = float(np.mean(tau_cf))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    ax.scatter(idx, tau_plr, s=6, alpha=0.4, label=f\"IATE PLR-DML (mean = {mean_plr:.2f})\")\n",
    "    ax.scatter(idx, tau_cf, s=6, alpha=0.4, label=f\"IATE CF-DML (mean = {mean_cf:.2f})\")\n",
    "\n",
    "    ax.axhline(mean_plr, linestyle=\"--\", linewidth=1)\n",
    "    ax.axhline(mean_cf, linestyle=\":\", linewidth=1)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Observation (trimmed sample)\")\n",
    "    ax.set_ylabel(\"Individual treatment effect (minutes)\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"iate_scatter__{_safe_name(t_col)}.png\", subdir=\"iates\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_robustness_frontier(\n",
    "    t_col,\n",
    "    beta_hat,\n",
    "    se_hat,\n",
    "    df_resid,\n",
    "    partial_R2_TY_given_X,\n",
    "    R2_Y_X,\n",
    "    R2_T_X,\n",
    "    max_r2=0.2,\n",
    "    levels_t=(0.0, 2.0)\n",
    "):\n",
    "    r2_tu_vals = np.linspace(0.0, max_r2, 101)\n",
    "    r2_yu_vals = np.linspace(0.0, max_r2, 101)\n",
    "    R2_TU, R2_YU = np.meshgrid(r2_tu_vals, r2_yu_vals)\n",
    "\n",
    "    beta_adj = compute_bias_adjusted(beta_hat, se_hat, df_resid, R2_YU, R2_TU)\n",
    "    t_adj = beta_adj / se_hat\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 5))\n",
    "\n",
    "    cf = ax.contourf(R2_TU, R2_YU, np.abs(t_adj), levels=30, alpha=0.85)\n",
    "    cbar = fig.colorbar(cf, ax=ax)\n",
    "    cbar.set_label(r\"$|t_{\\mathrm{adj}}|$\")\n",
    "\n",
    "    for lev in levels_t:\n",
    "        cs = ax.contour(R2_TU, R2_YU, np.abs(t_adj), levels=[lev], colors=\"k\",\n",
    "                        linestyles=\"--\" if lev == 2.0 else \"-.\")\n",
    "        ax.clabel(cs, inline=True, fontsize=9, fmt={lev: rf\"$|t|={lev:.0f}$\"})\n",
    "\n",
    "    if np.isfinite(partial_R2_TY_given_X):\n",
    "        ax.axhline(partial_R2_TY_given_X, color=\"red\", linestyle=\":\", lw=1.5,\n",
    "                   label=rf\"$R^2_{{Y\\sim T|X}}={partial_R2_TY_given_X:.3f}$\")\n",
    "\n",
    "    if np.isfinite(R2_T_X):\n",
    "        ax.axvline(R2_T_X, color=\"blue\", linestyle=\":\", lw=1.5,\n",
    "                   label=rf\"$R^2_{{T\\sim X}}={R2_T_X:.3f}$\")\n",
    "\n",
    "    ax.set_xlabel(r\"$R^2_{T\\sim U\\,|\\,X}$  (strength of confounder with treatment)\")\n",
    "    ax.set_ylabel(r\"$R^2_{Y\\sim U\\,|\\,T,X}$  (strength of confounder with outcome)\")\n",
    "    ax.set_title(f\"Sensitivity frontier – {t_col}\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_fig(fig, f\"robustness_frontier__{_safe_name(t_col)}.png\", subdir=\"sensitivity\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_delay_distributions(Y_trim, T_trim, t_col):\n",
    "    Y_trim = np.asarray(Y_trim, dtype=float)\n",
    "    T_trim = np.asarray(T_trim, dtype=int)\n",
    "\n",
    "    y_t = Y_trim[T_trim == 1]\n",
    "    y_c = Y_trim[T_trim == 0]\n",
    "\n",
    "    if len(y_t) == 0 or len(y_c) == 0:\n",
    "        print(f\"[plot_delay_distributions] No treated or control for {t_col}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    ax = axes[0]\n",
    "    bins = 40\n",
    "    ax.hist(y_c, bins=bins, alpha=0.6, density=True, label=\"Control\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.hist(y_t, bins=bins, alpha=0.6, density=True, label=\"Treated\", edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.set_title(f\"Delay distribution – {t_col}\")\n",
    "    ax.set_xlabel(\"Δ arrival (minutes)\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.boxplot([y_c, y_t], labels=[\"Control\", \"Treated\"], showmeans=True)\n",
    "    ax.set_title(f\"Boxplot – {t_col}\")\n",
    "    ax.set_ylabel(\"Δ arrival (minutes)\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_fig(fig, f\"delay_distributions__{_safe_name(t_col)}.png\", subdir=\"robustness\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_robust_effects_bar(robust_stats, qte_df, t_col):\n",
    "    if robust_stats is None or len(robust_stats) == 0:\n",
    "        print(f\"[plot_robust_effects_bar] No stats for {t_col}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    effects, labels = [], []\n",
    "\n",
    "    for k, lab in [\n",
    "        (\"diff_mean\", \"Mean\"),\n",
    "        (\"diff_median\", \"Median\"),\n",
    "        (\"diff_trimmed_5\", \"Trimmed 5%\"),\n",
    "        (\"diff_winsor_1\", \"Winsor 1%\"),\n",
    "        (\"diff_winsor_2\", \"Winsor 2%\"),\n",
    "    ]:\n",
    "        if k in robust_stats and np.isfinite(robust_stats[k]):\n",
    "            labels.append(lab)\n",
    "            effects.append(float(robust_stats[k]))\n",
    "\n",
    "    if qte_df is not None and not qte_df.empty:\n",
    "        for tau_target in [0.5, 0.9]:\n",
    "            row = qte_df[np.isclose(qte_df[\"tau\"], tau_target)]\n",
    "            if not row.empty:\n",
    "                labels.append(f\"QTE τ={tau_target:.2f}\")\n",
    "                effects.append(float(row[\"QTE\"].iloc[0]))\n",
    "\n",
    "    if len(effects) == 0:\n",
    "        print(f\"[plot_robust_effects_bar] No finite effects to plot for {t_col}.\")\n",
    "        return None\n",
    "\n",
    "    x = np.arange(len(effects))\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.bar(x, effects, edgecolor=\"black\", linewidth=0.4)\n",
    "    ax.axhline(0.0, color=\"black\", linewidth=0.8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=0)\n",
    "    ax.set_ylabel(\"Treatment effect on Δ arrival (minutes)\")\n",
    "    ax.set_title(f\"Robustness summary – {t_col}\\n(positive = more delay)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_fig(fig, f\"robust_effects__{_safe_name(t_col)}.png\", subdir=\"robustness\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 8.2 Patch cluster multi-seed to also save CSV\n",
    "# ----------------------------\n",
    "def plr_dml_cluster_multi_seed(\n",
    "    Y_trim,\n",
    "    T_trim,\n",
    "    X_trim,\n",
    "    cluster_ids,\n",
    "    n_folds=3,\n",
    "    seeds=(1, 11, 21, 31, 41),\n",
    "    rf_y_params=None,\n",
    "    rf_t_params=None,\n",
    "    t_col=\"treatment\"\n",
    "):\n",
    "    Y = np.asarray(Y_trim, dtype=float)\n",
    "    T = np.asarray(T_trim, dtype=float)\n",
    "    X = np.asarray(X_trim, dtype=float)\n",
    "    clusters = np.asarray(cluster_ids)\n",
    "\n",
    "    if rf_y_params is None:\n",
    "        rf_y_params = dict(n_estimators=300, max_depth=12, min_samples_leaf=50, random_state=None, n_jobs=-1)\n",
    "    if rf_t_params is None:\n",
    "        rf_t_params = dict(n_estimators=300, max_depth=8, min_samples_leaf=50, random_state=None, n_jobs=-1)\n",
    "\n",
    "    records = []\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n[PLR cluster] Running residualization for seed={seed} ...\")\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        Y_tilde = np.zeros_like(Y, dtype=float)\n",
    "        T_tilde = np.zeros_like(T, dtype=float)\n",
    "\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            Y_train = Y[train_idx]\n",
    "            T_train = T[train_idx]\n",
    "\n",
    "            my = RandomForestRegressor(**{**rf_y_params, \"random_state\": seed})\n",
    "            my.fit(X_train, Y_train)\n",
    "            mY_test = my.predict(X_test)\n",
    "\n",
    "            mt = RandomForestClassifier(**{**rf_t_params, \"random_state\": seed})\n",
    "            mt.fit(X_train, T_train)\n",
    "            mT_test = mt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            Y_tilde[test_idx] = Y[test_idx] - mY_test\n",
    "            T_tilde[test_idx] = T[test_idx] - mT_test\n",
    "\n",
    "        X_ols = sm.add_constant(T_tilde)\n",
    "        model = sm.OLS(Y_tilde, X_ols)\n",
    "        res = model.fit(cov_type=\"cluster\", cov_kwds={\"groups\": clusters})\n",
    "\n",
    "        beta = float(res.params[1])\n",
    "        se_cl = float(res.bse[1])\n",
    "        t_val = float(res.tvalues[1])\n",
    "        p_val = float(res.pvalues[1])\n",
    "        ci = res.conf_int()\n",
    "        ci_low = float(ci[1, 0])\n",
    "        ci_high = float(ci[1, 1])\n",
    "\n",
    "        print(f\"[PLR cluster seed={seed}] beta={beta:.4f}, SE_cluster={se_cl:.4f}, \"\n",
    "              f\"CI=[{ci_low:.4f}, {ci_high:.4f}], p={p_val:.4f}\")\n",
    "\n",
    "        records.append({\n",
    "            \"seed\": seed,\n",
    "            \"beta_cluster\": beta,\n",
    "            \"SE_cluster\": se_cl,\n",
    "            \"CI_low_cluster\": ci_low,\n",
    "            \"CI_high_cluster\": ci_high,\n",
    "            \"t_cluster\": t_val,\n",
    "            \"p_cluster\": p_val\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(records)\n",
    "    summary = {\n",
    "        \"beta_cluster_mean\": float(results_df[\"beta_cluster\"].mean()),\n",
    "        \"beta_cluster_sd\": float(results_df[\"beta_cluster\"].std()),\n",
    "        \"beta_cluster_min\": float(results_df[\"beta_cluster\"].min()),\n",
    "        \"beta_cluster_max\": float(results_df[\"beta_cluster\"].max()),\n",
    "    }\n",
    "\n",
    "    save_table(results_df, f\"cluster_plr_multiseed__{_safe_name(t_col)}\", to_latex=False, index=False)\n",
    "\n",
    "    print(\"\\n[PLR cluster] Stability summary across seeds:\")\n",
    "    print(results_df)\n",
    "    print(\"Mean beta_cluster:\", summary[\"beta_cluster_mean\"])\n",
    "    print(\"SD beta_cluster  :\", summary[\"beta_cluster_sd\"])\n",
    "    return results_df, summary\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8.3 Core routine: PLR-DML + CF-DML + GATE + Sensitivity + Robustness + SMD + ECDF + Cluster-SE + Stability\n",
    "#     (REVISED: saves alpha table, SMD, GATE tables, QTE, cluster tables, and summary tables)\n",
    "# ============================================================\n",
    "\n",
    "def run_binary_plr_cf_with_gate(\n",
    "    df,\n",
    "    Y_COL,\n",
    "    t_col,\n",
    "    covariates,\n",
    "    num_features,\n",
    "    cat_features,\n",
    "    alphas_grid=None,\n",
    "    min_treated=100,\n",
    "    min_control=100,\n",
    "    max_sample=30000,\n",
    "    cluster_col=None\n",
    "):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"Binary PLR-DML & CF-DML with GATE, SMD, sensitivity, cluster-SE for treatment: {t_col}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    if alphas_grid is None:\n",
    "        alphas_grid = np.round(np.linspace(0.0, 0.20, 21), 3)\n",
    "\n",
    "    needed_cols = [Y_COL, t_col] + covariates\n",
    "    if cluster_col is not None and cluster_col not in needed_cols:\n",
    "        needed_cols.append(cluster_col)\n",
    "    needed_cols = list(dict.fromkeys(needed_cols))\n",
    "\n",
    "    df_t_full = df[needed_cols].copy()\n",
    "    df_t_full[t_col] = df_t_full[t_col].fillna(0).astype(int)\n",
    "\n",
    "    if (max_sample is not None) and (len(df_t_full) > max_sample):\n",
    "        idx_sample = np.random.choice(len(df_t_full), size=max_sample, replace=False)\n",
    "        df_work = df_t_full.iloc[idx_sample].reset_index(drop=True)\n",
    "        print(f\"Sampled down to {max_sample} observations for {t_col} to control runtime.\")\n",
    "    else:\n",
    "        df_work = df_t_full.reset_index(drop=True)\n",
    "\n",
    "    Y = df_work[Y_COL].astype(float).values\n",
    "    T = df_work[t_col].values\n",
    "\n",
    "    # ----- Prepare X (impute + one-hot) -----\n",
    "    X_raw = df_work[covariates].copy()\n",
    "\n",
    "    local_num_feats = [c for c in num_features if c in X_raw.columns]\n",
    "    for c in local_num_feats:\n",
    "        X_raw[c] = pd.to_numeric(X_raw[c], errors=\"coerce\")\n",
    "        med = X_raw[c].median()\n",
    "        X_raw[c] = X_raw[c].fillna(med)\n",
    "\n",
    "    local_cat_feats = [c for c in cat_features if c in X_raw.columns]\n",
    "    for c in local_cat_feats:\n",
    "        X_raw[c] = X_raw[c].astype(\"object\").fillna(\"Missing\")\n",
    "\n",
    "    X_enc = pd.get_dummies(X_raw, columns=local_cat_feats, drop_first=False)\n",
    "    X_enc = X_enc.reset_index(drop=True)\n",
    "    X_mat = X_enc.values.astype(\"float32\")\n",
    "\n",
    "    print(f\"{t_col}: After cleaning & imputation\")\n",
    "    print(f\"  N       = {len(Y)}\")\n",
    "    print(f\"  Treated = {int(T.sum())} | Control = {int(len(T) - T.sum())}\")\n",
    "    print(f\"  X shape = {X_mat.shape}\")\n",
    "\n",
    "    if len(np.unique(T)) < 2:\n",
    "        print(f\"{t_col}: Only one class present in treatment. Skipping.\")\n",
    "        return {\n",
    "            \"treatment\": t_col,\n",
    "            \"alpha\": np.nan,\n",
    "            \"N\": len(Y),\n",
    "            \"N_treated\": int(T.sum()),\n",
    "            \"N_control\": int(len(T) - T.sum()),\n",
    "            \"ATE_DML\": np.nan,\n",
    "            \"CI_DML\": (np.nan, np.nan),\n",
    "            \"ATE_CF_DML\": np.nan,\n",
    "            \"CI_CF_DML\": (np.nan, np.nan),\n",
    "        }\n",
    "\n",
    "    # ----- Propensity model -----\n",
    "    logit = LogisticRegression(solver=\"lbfgs\", penalty=\"l2\", max_iter=1000, random_state=RANDOM_STATE)\n",
    "    param_grid = {\"C\": [0.1, 1.0]}\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=logit,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"neg_log_loss\",\n",
    "        cv=3,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    grid.fit(X_mat, T)\n",
    "    prop_model = grid.best_estimator_\n",
    "    print(f\"Best propensity model C: {prop_model.C}\")\n",
    "\n",
    "    pscores = prop_model.predict_proba(X_mat)[:, 1]\n",
    "    print(f\"{t_col}: Propensity score summary (before trimming):\")\n",
    "    print(pd.Series(pscores).describe())\n",
    "\n",
    "    plot_propensity_hist(T, pscores, t_col)\n",
    "\n",
    "    alpha_star, alpha_table = choose_alpha_crump(pscores, T, alphas=alphas_grid,\n",
    "                                                 min_treated=min_treated, min_control=min_control)\n",
    "    if alpha_table is not None and not alpha_table.empty:\n",
    "        save_table(alpha_table, f\"alpha_crump_table__{_safe_name(t_col)}\", to_latex=False, index=False)\n",
    "\n",
    "    mask_trim = (pscores >= alpha_star) & (pscores <= 1 - alpha_star)\n",
    "    X_trim = X_mat[mask_trim, :]\n",
    "    Y_trim = Y[mask_trim]\n",
    "    T_trim = T[mask_trim]\n",
    "    pscores_trim = pscores[mask_trim]\n",
    "\n",
    "    if cluster_col is not None and cluster_col in df_work.columns:\n",
    "        cluster_ids_trim = df_work.loc[mask_trim, cluster_col].values\n",
    "    else:\n",
    "        cluster_ids_trim = np.arange(len(Y_trim))\n",
    "\n",
    "    plot_propensity_before_after(T, pscores, mask_trim, alpha_star, t_col)\n",
    "    plot_ps_ecdf_before_after(T, pscores, mask_trim, alpha_star, t_col)\n",
    "\n",
    "    print(f\"\\n{t_col}: After trimming with alpha*={alpha_star:.3f}\")\n",
    "    print(f\"  N       = {len(Y_trim)}\")\n",
    "    print(f\"  Treated = {int(T_trim.sum())} | Control = {int(len(T_trim) - T_trim.sum())}\")\n",
    "    print(f\"  X shape = {X_trim.shape}\")\n",
    "\n",
    "    if len(np.unique(T_trim)) < 2:\n",
    "        print(f\"{t_col}: Only one class after trimming. Skipping.\")\n",
    "        return {\n",
    "            \"treatment\": t_col,\n",
    "            \"alpha\": alpha_star,\n",
    "            \"N\": len(Y_trim),\n",
    "            \"N_treated\": int(T_trim.sum()),\n",
    "            \"N_control\": int(len(T_trim) - T_trim.sum()),\n",
    "            \"ATE_DML\": np.nan,\n",
    "            \"CI_DML\": (np.nan, np.nan),\n",
    "            \"ATE_CF_DML\": np.nan,\n",
    "            \"CI_CF_DML\": (np.nan, np.nan),\n",
    "        }\n",
    "\n",
    "    # ----- SMD before/after -----\n",
    "    df_before_smd = pd.concat([pd.Series(T, name=t_col),\n",
    "                               pd.DataFrame(X_mat, columns=X_enc.columns)], axis=1)\n",
    "    df_after_smd = df_before_smd.loc[mask_trim].copy()\n",
    "    cov_smd = X_enc.columns.tolist()\n",
    "\n",
    "    smd_table = summarize_smd_before_after(df_before_smd, df_after_smd, treat_col=t_col, covariates=cov_smd)\n",
    "\n",
    "    print(\"\\n=== SMD balance (top 20 covariates by |SMD| before trimming) –\", t_col, \"===\")\n",
    "    print(smd_table.head(20))\n",
    "\n",
    "    save_table(smd_table, f\"smd_before_after__{_safe_name(t_col)}\", to_latex=True, index=False)\n",
    "\n",
    "    max_abs_SMD_before = smd_table[\"abs_SMD_before\"].max()\n",
    "    max_abs_SMD_after = smd_table[\"abs_SMD_after\"].max()\n",
    "    n_cov_SMD_gt_0_1_before = (smd_table[\"abs_SMD_before\"] > 0.1).sum()\n",
    "    n_cov_SMD_gt_0_1_after = (smd_table[\"abs_SMD_after\"] > 0.1).sum()\n",
    "    n_cov_SMD_gt_0_2_before = (smd_table[\"abs_SMD_before\"] > 0.2).sum()\n",
    "    n_cov_SMD_gt_0_2_after = (smd_table[\"abs_SMD_after\"] > 0.2).sum()\n",
    "\n",
    "    print(f\"\\nSMD summary – {t_col}:\")\n",
    "    print(f\"  Max |SMD| before trimming: {max_abs_SMD_before:.3f}\")\n",
    "    print(f\"  Max |SMD| after  trimming: {max_abs_SMD_after:.3f}\")\n",
    "    print(f\"  #covariates |SMD|>0.10 before: {n_cov_SMD_gt_0_1_before}\")\n",
    "    print(f\"  #covariates |SMD|>0.10 after : {n_cov_SMD_gt_0_1_after}\")\n",
    "    print(f\"  #covariates |SMD|>0.20 before: {n_cov_SMD_gt_0_2_before}\")\n",
    "    print(f\"  #covariates |SMD|>0.20 after : {n_cov_SMD_gt_0_2_after}\")\n",
    "\n",
    "    # ----- econml PLR-DML & CF-DML -----\n",
    "    model_y = RandomForestRegressor(\n",
    "        n_estimators=300, max_depth=12, min_samples_leaf=50,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    model_t = RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=8, min_samples_leaf=50,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    dml = LinearDML(model_y=model_y, model_t=model_t, discrete_treatment=True, cv=3, random_state=RANDOM_STATE)\n",
    "    dml.fit(Y_trim, T_trim, X=X_trim)\n",
    "\n",
    "    ate_dml = float(dml.ate(X=X_trim))\n",
    "    ci_low_dml, ci_high_dml = dml.ate_interval(X=X_trim, alpha=0.05)\n",
    "    ci_low_dml, ci_high_dml = float(ci_low_dml), float(ci_high_dml)\n",
    "    tau_plr = np.ravel(dml.effect(X_trim))\n",
    "\n",
    "    print(f\"\\n{t_col} – PLR-DML (econml) ATE: {ate_dml:.3f} min, 95% CI [{ci_low_dml:.3f}, {ci_high_dml:.3f}]\")\n",
    "\n",
    "    cf_dml = CausalForestDML(\n",
    "        model_y=model_y, model_t=model_t, discrete_treatment=True,\n",
    "        n_estimators=1000, min_samples_leaf=50, max_depth=None,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    cf_dml.fit(Y_trim, T_trim, X=X_trim)\n",
    "\n",
    "    tau_cf = np.ravel(cf_dml.effect(X_trim))\n",
    "    ate_cf_dml = float(np.mean(tau_cf))\n",
    "    ci_low_cf, ci_high_cf = cf_dml.ate_interval(X=X_trim, alpha=0.05)\n",
    "    ci_low_cf, ci_high_cf = float(ci_low_cf), float(ci_high_cf)\n",
    "\n",
    "    print(f\"{t_col} – CF-DML ATE: {ate_cf_dml:.3f} min, 95% CI [{ci_low_cf:.3f}, {ci_high_cf:.3f}]\")\n",
    "\n",
    "    # ----- Sensitivity -----\n",
    "    sens = run_partial_r2_sensitivity(Y_trim, T_trim, X_trim, t_col)\n",
    "    sens_df = pd.DataFrame([sens])\n",
    "    save_table(sens_df, f\"sensitivity_stats__{_safe_name(t_col)}\", to_latex=False, index=False)\n",
    "\n",
    "    rv_zero = cinelli_robustness_value(t_stat=sens[\"t_T_ols\"], df_resid=sens[\"df_resid\"], q=1.0, alpha=1.0)\n",
    "    print(f\"\\nCinelli/Hazlett robustness value RV_zero for {t_col}: {rv_zero:.4f}\")\n",
    "\n",
    "    # ----- Robustness: distributions + simple stats + QTE -----\n",
    "    print(f\"\\n=== Robustness checks on outcome distribution – {t_col} ===\")\n",
    "    plot_delay_distributions(Y_trim, T_trim, t_col)\n",
    "\n",
    "    robust_stats, qte_df = compute_robust_effects(Y_trim, T_trim, taus=(0.5, 0.75, 0.9))\n",
    "\n",
    "    # save robust tables\n",
    "    if robust_stats is not None and len(robust_stats) > 0:\n",
    "        robust_stats_df = pd.DataFrame([robust_stats])\n",
    "        save_table(robust_stats_df, f\"robust_summary_stats__{_safe_name(t_col)}\", to_latex=False, index=False)\n",
    "\n",
    "    if qte_df is not None and not qte_df.empty:\n",
    "        save_table(qte_df, f\"robust_qte__{_safe_name(t_col)}\", to_latex=False, index=False)\n",
    "\n",
    "    plot_robust_effects_bar(robust_stats, qte_df, t_col)\n",
    "\n",
    "    # ----- IATE plot -----\n",
    "    plot_iate_scatter(tau_plr, tau_cf, title=f\"IATEs – {t_col} (PLR-DML vs CF-DML)\", t_col=t_col)\n",
    "\n",
    "    # ----- GATEs + Holm (PLR) -----\n",
    "    df_trim = df_work.loc[mask_trim].reset_index(drop=True)\n",
    "    df_gate = df_trim.copy()\n",
    "    df_gate[\"IATE_PLR\"] = tau_plr\n",
    "    df_gate[\"IATE_CF\"] = tau_cf\n",
    "\n",
    "    gate_results = {}\n",
    "\n",
    "    if \"Line_Original\" in df_gate.columns:\n",
    "        gate_line = adjust_gate_pvalues(compute_gate_two(df_gate, \"Line_Original\"))\n",
    "        gate_results[\"line\"] = gate_line\n",
    "        print(\"\\n=== GATE by Line_Original (PLR & CF, Holm-adjusted p) ===\")\n",
    "        print(gate_line)\n",
    "        save_table(gate_line.reset_index(), f\"gate_line__{_safe_name(t_col)}\", to_latex=True, index=False)\n",
    "        plot_gate_two_bars(gate_line, \"Line_Original\", f\"GATE by Line – {t_col}\", t_col=t_col)\n",
    "\n",
    "    if \"Weekday\" in df_gate.columns:\n",
    "        gate_wd = adjust_gate_pvalues(compute_gate_two(df_gate, \"Weekday\"))\n",
    "        gate_results[\"weekday\"] = gate_wd\n",
    "        print(\"\\n=== GATE by Weekday (PLR & CF, Holm-adjusted p) ===\")\n",
    "        print(gate_wd)\n",
    "        save_table(gate_wd.reset_index(), f\"gate_weekday__{_safe_name(t_col)}\", to_latex=True, index=False)\n",
    "        plot_gate_two_bars(gate_wd, \"Weekday\", f\"GATE by Weekday – {t_col}\", t_col=t_col, is_weekday=True)\n",
    "\n",
    "    if \"Month\" in df_gate.columns:\n",
    "        gate_month = adjust_gate_pvalues(compute_gate_two(df_gate, \"Month\"))\n",
    "        gate_results[\"month\"] = gate_month\n",
    "        print(\"\\n=== GATE by Month (PLR & CF, Holm-adjusted p) ===\")\n",
    "        print(gate_month)\n",
    "        save_table(gate_month.reset_index(), f\"gate_month__{_safe_name(t_col)}\", to_latex=True, index=False)\n",
    "        if gate_month.shape[0] >= 2:\n",
    "            plot_gate_two_bars(gate_month, \"Month\", f\"GATE by Month – {t_col}\", t_col=t_col, is_month=True)\n",
    "        else:\n",
    "            print(\"Skipping month GATE plot: trimmed sample contains only one month.\")\n",
    "\n",
    "    if \"Hour\" in df_gate.columns:\n",
    "        gate_hour = adjust_gate_pvalues(compute_gate_two(df_gate, \"Hour\"))\n",
    "        gate_results[\"hour\"] = gate_hour\n",
    "        print(\"\\n=== GATE by Hour (PLR & CF, Holm-adjusted p) ===\")\n",
    "        print(gate_hour)\n",
    "        save_table(gate_hour.reset_index(), f\"gate_hour__{_safe_name(t_col)}\", to_latex=True, index=False)\n",
    "        plot_gate_two_bars(gate_hour, \"Hour\", f\"GATE by Hour – {t_col}\", t_col=t_col)\n",
    "\n",
    "    # ----- Robustness frontier -----\n",
    "    if np.isfinite(sens[\"beta_T_ols\"]) and np.isfinite(sens[\"se_T_ols\"]):\n",
    "        plot_robustness_frontier(\n",
    "            t_col=t_col,\n",
    "            beta_hat=sens[\"beta_T_ols\"],\n",
    "            se_hat=sens[\"se_T_ols\"],\n",
    "            df_resid=sens[\"df_resid\"],\n",
    "            partial_R2_TY_given_X=sens[\"partial_R2_TY_given_X\"],\n",
    "            R2_Y_X=sens[\"R2_Y_X\"],\n",
    "            R2_T_X=sens[\"R2_T_X\"],\n",
    "            max_r2=0.2,\n",
    "            levels_t=(0.0, 2.0)\n",
    "        )\n",
    "\n",
    "    # ----- Cluster-robust PLR-DML + multi-seed stability -----\n",
    "    cluster_results_df, cluster_summary = plr_dml_cluster_multi_seed(\n",
    "        Y_trim=Y_trim,\n",
    "        T_trim=T_trim,\n",
    "        X_trim=X_trim,\n",
    "        cluster_ids=cluster_ids_trim,\n",
    "        n_folds=3,\n",
    "        seeds=(1, 11, 21, 31, 41),\n",
    "        rf_y_params=dict(n_estimators=300, max_depth=12, min_samples_leaf=50, n_jobs=-1),\n",
    "        rf_t_params=dict(n_estimators=300, max_depth=8, min_samples_leaf=50, n_jobs=-1),\n",
    "        t_col=t_col\n",
    "    )\n",
    "\n",
    "    out = {\n",
    "        \"treatment\": t_col,\n",
    "        \"alpha\": float(alpha_star),\n",
    "        \"N\": int(len(Y_trim)),\n",
    "        \"N_treated\": int(T_trim.sum()),\n",
    "        \"N_control\": int(len(T_trim) - T_trim.sum()),\n",
    "        \"ATE_DML\": float(ate_dml),\n",
    "        \"CI_DML\": (float(ci_low_dml), float(ci_high_dml)),\n",
    "        \"ATE_CF_DML\": float(ate_cf_dml),\n",
    "        \"CI_CF_DML\": (float(ci_low_cf), float(ci_high_cf)),\n",
    "        \"beta_T_ols\": sens[\"beta_T_ols\"],\n",
    "        \"se_T_ols\": sens[\"se_T_ols\"],\n",
    "        \"t_T_ols\": sens[\"t_T_ols\"],\n",
    "        \"p_T_ols\": sens[\"p_T_ols\"],\n",
    "        \"partial_R2_TY_given_X\": sens[\"partial_R2_TY_given_X\"],\n",
    "        \"R2_Y_X\": sens[\"R2_Y_X\"],\n",
    "        \"R2_T_X\": sens[\"R2_T_X\"],\n",
    "        \"df_resid\": sens[\"df_resid\"],\n",
    "        \"RV_zero\": float(rv_zero),\n",
    "        \"max_abs_SMD_before\": float(max_abs_SMD_before),\n",
    "        \"max_abs_SMD_after\": float(max_abs_SMD_after),\n",
    "        \"n_cov_SMD_gt_0_1_before\": int(n_cov_SMD_gt_0_1_before),\n",
    "        \"n_cov_SMD_gt_0_1_after\": int(n_cov_SMD_gt_0_1_after),\n",
    "        \"n_cov_SMD_gt_0_2_before\": int(n_cov_SMD_gt_0_2_before),\n",
    "        \"n_cov_SMD_gt_0_2_after\": int(n_cov_SMD_gt_0_2_after),\n",
    "        \"beta_cluster_mean\": float(cluster_summary[\"beta_cluster_mean\"]),\n",
    "        \"beta_cluster_sd\": float(cluster_summary[\"beta_cluster_sd\"]),\n",
    "        \"beta_cluster_min\": float(cluster_summary[\"beta_cluster_min\"]),\n",
    "        \"beta_cluster_max\": float(cluster_summary[\"beta_cluster_max\"]),\n",
    "        \"diff_mean\": float(robust_stats.get(\"diff_mean\", np.nan)) if robust_stats else np.nan,\n",
    "        \"diff_median\": float(robust_stats.get(\"diff_median\", np.nan)) if robust_stats else np.nan,\n",
    "        \"diff_trimmed_5\": float(robust_stats.get(\"diff_trimmed_5\", np.nan)) if robust_stats else np.nan,\n",
    "        \"diff_winsor_1\": float(robust_stats.get(\"diff_winsor_1\", np.nan)) if robust_stats else np.nan,\n",
    "        \"diff_winsor_2\": float(robust_stats.get(\"diff_winsor_2\", np.nan)) if robust_stats else np.nan,\n",
    "        \"cluster_results_df\": cluster_results_df,\n",
    "        \"gate_results\": gate_results,\n",
    "    }\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. Run pipeline for all treatments and SAVE final summary tables\n",
    "# ============================================================\n",
    "\n",
    "treatments = [\"T_temp_severe\", \"T_wind_severe\", \"T_rain_severe\", \"Snow_Any\"]\n",
    "covariates = base_covariates\n",
    "alphas_grid = np.round(np.linspace(0.0, 0.20, 21), 3)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for t_col in treatments:\n",
    "    res = run_binary_plr_cf_with_gate(\n",
    "        df=df,\n",
    "        Y_COL=Y_COL,\n",
    "        t_col=t_col,\n",
    "        covariates=covariates,\n",
    "        num_features=num_features,\n",
    "        cat_features=cat_features,\n",
    "        alphas_grid=alphas_grid,\n",
    "        min_treated=100,\n",
    "        min_control=100,\n",
    "        max_sample=30000,\n",
    "        cluster_col=cluster_col\n",
    "    )\n",
    "    all_results.append(res)\n",
    "\n",
    "# ---------- CORE SUMMARY TABLE ----------\n",
    "core_rows = []\n",
    "for res in all_results:\n",
    "    core_rows.append({\n",
    "        \"treatment\": res[\"treatment\"],\n",
    "        \"alpha\": res[\"alpha\"],\n",
    "        \"N\": res[\"N\"],\n",
    "        \"N_treated\": res[\"N_treated\"],\n",
    "        \"N_control\": res[\"N_control\"],\n",
    "        \"ATE_DML\": res[\"ATE_DML\"],\n",
    "        \"CI_DML_low\": res[\"CI_DML\"][0],\n",
    "        \"CI_DML_high\": res[\"CI_DML\"][1],\n",
    "        \"ATE_CF_DML\": res[\"ATE_CF_DML\"],\n",
    "        \"CI_CF_DML_low\": res[\"CI_CF_DML\"][0],\n",
    "        \"CI_CF_DML_high\": res[\"CI_CF_DML\"][1],\n",
    "        \"beta_T_ols\": res.get(\"beta_T_ols\", np.nan),\n",
    "        \"t_T_ols\": res.get(\"t_T_ols\", np.nan),\n",
    "        \"p_T_ols\": res.get(\"p_T_ols\", np.nan),\n",
    "        \"partial_R2_TY_given_X\": res.get(\"partial_R2_TY_given_X\", np.nan),\n",
    "        \"R2_Y_X\": res.get(\"R2_Y_X\", np.nan),\n",
    "        \"R2_T_X\": res.get(\"R2_T_X\", np.nan),\n",
    "        \"RV_zero\": res.get(\"RV_zero\", np.nan),\n",
    "        \"diff_mean\": res.get(\"diff_mean\", np.nan),\n",
    "        \"diff_median\": res.get(\"diff_median\", np.nan),\n",
    "        \"diff_trimmed_5\": res.get(\"diff_trimmed_5\", np.nan),\n",
    "        \"diff_winsor_1\": res.get(\"diff_winsor_1\", np.nan),\n",
    "        \"diff_winsor_2\": res.get(\"diff_winsor_2\", np.nan),\n",
    "    })\n",
    "\n",
    "summary_core = pd.DataFrame(core_rows)\n",
    "print(\"\\n\\n==== SUMMARY OF CAUSAL EFFECT ESTIMATES + OLS SENSITIVITY + ROBUSTNESS ====\")\n",
    "print(summary_core)\n",
    "\n",
    "save_table(summary_core, \"summary_core_weather\", to_latex=True, index=False)\n",
    "\n",
    "# ---------- EXTENDED SUMMARY TABLE ----------\n",
    "extended_rows = []\n",
    "for res in all_results:\n",
    "    extended_rows.append({\n",
    "        \"treatment\": res[\"treatment\"],\n",
    "        \"alpha\": res[\"alpha\"],\n",
    "        \"N\": res[\"N\"],\n",
    "        \"N_treated\": res[\"N_treated\"],\n",
    "        \"N_control\": res[\"N_control\"],\n",
    "        \"ATE_DML\": res[\"ATE_DML\"],\n",
    "        \"CI_DML_low\": res[\"CI_DML\"][0],\n",
    "        \"CI_DML_high\": res[\"CI_DML\"][1],\n",
    "        \"ATE_CF_DML\": res[\"ATE_CF_DML\"],\n",
    "        \"CI_CF_DML_low\": res[\"CI_CF_DML\"][0],\n",
    "        \"CI_CF_DML_high\": res[\"CI_CF_DML\"][1],\n",
    "        \"beta_T_ols\": res.get(\"beta_T_ols\", np.nan),\n",
    "        \"t_T_ols\": res.get(\"t_T_ols\", np.nan),\n",
    "        \"p_T_ols\": res.get(\"p_T_ols\", np.nan),\n",
    "        \"partial_R2_TY_given_X\": res.get(\"partial_R2_TY_given_X\", np.nan),\n",
    "        \"R2_Y_X\": res.get(\"R2_Y_X\", np.nan),\n",
    "        \"R2_T_X\": res.get(\"R2_T_X\", np.nan),\n",
    "        \"RV_zero\": res.get(\"RV_zero\", np.nan),\n",
    "        \"diff_mean\": res.get(\"diff_mean\", np.nan),\n",
    "        \"diff_median\": res.get(\"diff_median\", np.nan),\n",
    "        \"diff_trimmed_5\": res.get(\"diff_trimmed_5\", np.nan),\n",
    "        \"diff_winsor_1\": res.get(\"diff_winsor_1\", np.nan),\n",
    "        \"diff_winsor_2\": res.get(\"diff_winsor_2\", np.nan),\n",
    "        \"max_abs_SMD_before\": res.get(\"max_abs_SMD_before\", np.nan),\n",
    "        \"max_abs_SMD_after\": res.get(\"max_abs_SMD_after\", np.nan),\n",
    "        \"n_cov_SMD_gt_0_1_before\": res.get(\"n_cov_SMD_gt_0_1_before\", np.nan),\n",
    "        \"n_cov_SMD_gt_0_1_after\": res.get(\"n_cov_SMD_gt_0_1_after\", np.nan),\n",
    "        \"n_cov_SMD_gt_0_2_before\": res.get(\"n_cov_SMD_gt_0_2_before\", np.nan),\n",
    "        \"n_cov_SMD_gt_0_2_after\": res.get(\"n_cov_SMD_gt_0_2_after\", np.nan),\n",
    "        \"beta_cluster_mean\": res.get(\"beta_cluster_mean\", np.nan),\n",
    "        \"beta_cluster_sd\": res.get(\"beta_cluster_sd\", np.nan),\n",
    "        \"beta_cluster_min\": res.get(\"beta_cluster_min\", np.nan),\n",
    "        \"beta_cluster_max\": res.get(\"beta_cluster_max\", np.nan),\n",
    "    })\n",
    "\n",
    "summary_extended = pd.DataFrame(extended_rows)\n",
    "print(\"\\n\\n==== EXTENDED SUMMARY (incl. SMD + CLUSTER-SE) ====\")\n",
    "print(summary_extended)\n",
    "\n",
    "save_table(summary_extended, \"summary_extended_weather\", to_latex=True, index=False)\n",
    "\n",
    "print(\"\\nSaved outputs to:\")\n",
    "print(\" - Figures:\", FIG_DIR.resolve())\n",
    "print(\" - Tables :\", TAB_DIR.resolve())\n",
    "\n",
    "# ============================================================\n",
    "# 10) GitHub hygiene (recommended)\n",
    "#     - create placeholder files so empty folders are tracked\n",
    "#     - optionally write a short README into outputs/\n",
    "# ============================================================\n",
    "\n",
    "# 10.1 Ensure Git tracks empty dirs (optional but useful)\n",
    "for p in [OUTPUT_DIR, FIG_DIR, TAB_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    keep = p / \".gitkeep\"\n",
    "    if not keep.exists():\n",
    "        keep.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "# 10.2 Write a small README in outputs/ (optional)\n",
    "readme_path = OUTPUT_DIR / \"README.md\"\n",
    "if not readme_path.exists():\n",
    "    readme_path.write_text(\n",
    "        \"# Outputs\\n\\n\"\n",
    "        \"This folder is created automatically by the analysis pipeline.\\n\\n\"\n",
    "        \"- `figures/`: saved plots (`.png`)\\n\"\n",
    "        \"- `tables/`: saved results tables (`.csv`, and optional `.tex`)\\n\\n\"\n",
    "        \"You can delete this folder safely; it will be regenerated.\\n\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "print(\"\\nGitHub note:\")\n",
    "print(\"If you do NOT want to version-control outputs, add 'outputs/' to .gitignore.\")\n",
    "print(\"If you DO want to keep only folder structure, commit the .gitkeep files and ignore *.png/*.csv.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
